{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Defining what dtype to convert each column to\n",
    "#numberic columns are transformed after reading in\n",
    "dtype_dict = {'Product':\"category\",\n",
    "             'Consumer consent provided?': \"category\",\n",
    "             'Submitted via': \"category\",\n",
    "             'Copany response to consumer': \"category\",\n",
    "             'Consumer disputed?': \"category\"}\n",
    "\n",
    "#read in .csv file, dates are parsed into datetime objects. \n",
    "#The Complaint ID is Unique in every entry, so it can be used as index\n",
    "df = pd.read_csv('Consumer_Complaints.csv',\n",
    "                 index_col=['Complaint ID'],\n",
    "                 parse_dates=[\"Date received\",\"Date sent to company\"],\n",
    "                 dtype=dtype_dict)\n",
    "\n",
    "#This will replace ending '-' to 5 (average linespace of 10)\n",
    "regexReplaceDash = r\"(\\d+)(-)$\"\n",
    "df['ZIP code'] = df['ZIP code'].str.replace(regexReplaceDash, r'\\g<1>5')\n",
    "\n",
    "#This will change ending XX to 50 (average linespace of 100)\n",
    "regex_XX = r'(\\d{3})(XX)'\n",
    "df['ZIP code'] = df['ZIP code'].str.replace(regex_XX, r'\\g<1>50')\n",
    "\n",
    "#This will remove all other entries that are still not 5 digits\n",
    "regexRemove = r'\\D+'\n",
    "df['ZIP code'] = df['ZIP code'].replace(regexRemove, np.nan, regex=True)\n",
    "\n",
    "#imputes the mean for nan \n",
    "imputeMean = df['ZIP code'].astype(np.float).mean()\n",
    "df['ZIP code'] = df['ZIP code'].astype(np.float).fillna(imputeMean)\n",
    "\n",
    "#Transforming 2 unique valued col to float boolean\n",
    "booleanize = {'Yes': 1, 'No': 0}\n",
    "df['Timely response?'] = pd.Series(df['Timely response?'].map(booleanize), dtype = np.float)\n",
    "\n",
    "#function to apply to column to convert less common results to 'Other', as well as NaN\n",
    "def convertToOther(value, keepList):\n",
    "    if (value == ''):\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        return value if value in keepList else \"Other\"\n",
    "    \n",
    "#Lists top 23 value counts (allowed to exclude values), turns NaN to '' to others, converts to category dtype\n",
    "def cleanReduceConvert(df, column, blackList=[]):\n",
    "    keepList = []\n",
    "    for category in df[column].value_counts().head(23).index.tolist():\n",
    "        if (category.lower().split()[0] != \"other\"):\n",
    "            keepList.append(category)\n",
    "    for category in blackList:\n",
    "        try:\n",
    "            keepList.remove(category)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    df[column].fillna('', inplace=True)\n",
    "    return pd.Series(df[column].apply(convertToOther, args=(keepList,)), dtype = 'category')\n",
    "\n",
    "df['Sub-product'] = cleanReduceConvert(df, 'Sub-product', blackList= ['I do not know'])\n",
    "df['Issue'] = cleanReduceConvert(df, 'Issue')\n",
    "df['Sub-issue'] = cleanReduceConvert(df, 'Sub-issue')\n",
    "df['Company'] = cleanReduceConvert(df, 'Company')\n",
    "\n",
    "def entryOrNull(strVal):\n",
    "    return 1.0 if strVal is not np.nan else 0.0\n",
    "\n",
    "df['Consumer complaint narrative submitted?'] = df['Consumer complaint narrative'].apply(entryOrNull)\n",
    "\n",
    "def dtToCols(df, dtcolumn):\n",
    "    df[\"{} day\".format(dtcolumn)] = df[dtcolumn].dt.day\n",
    "    df[\"{} month\".format(dtcolumn)] = df[dtcolumn].dt.month\n",
    "    df[\"{} year\".format(dtcolumn)] = df[dtcolumn].dt.year\n",
    "    \n",
    "dtToCols(df, \"Date received\")\n",
    "dtToCols(df, \"Date sent to company\")\n",
    "\n",
    "df[\"Consumer consent provided?\"] = df[\"Consumer consent provided?\"].cat.add_categories(\"Not recorded\").fillna(\"Not recorded\")\n",
    "\n",
    "df = df.drop(df[df[\"Company response to consumer\"].isna()].index)\n",
    "\n",
    "dfInProgress = df[df[\"Company response to consumer\"] == \"In progress\"]\n",
    "df = df[df[\"Company response to consumer\"] != \"In progress\"]\n",
    "\n",
    "dfUntimelyResponse = df[df[\"Company response to consumer\"] == \"Untimely response\"]\n",
    "df = df[df[\"Company response to consumer\"] != \"Untimely response\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#data columns not be used for the model\n",
    "dropList = [\"Consumer complaint narrative\",\n",
    "            \"Company public response\",\n",
    "            \"State\",\n",
    "            \"Tags\",\n",
    "            \"Consumer disputed?\",\n",
    "            \"Date received\", \n",
    "            \"Date sent to company\",\n",
    "            \"Company response to consumer\"]\n",
    "X = df.drop(dropList, axis=1)\n",
    "Y = df[\"Company response to consumer\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "#Columns to be standard scaled/imputed\n",
    "numeric_features = ['ZIP code',\n",
    "                    'Date received day',\n",
    "                    'Date received month',\n",
    "                    'Date received year',\n",
    "                    'Date sent to company day',\n",
    "                    'Date sent to company month',\n",
    "                    'Date sent to company year']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "#Columns to one hot encoded\n",
    "categorical_features = ['Product',\n",
    "           'Sub-product',\n",
    "           'Issue',\n",
    "           'Sub-issue',\n",
    "           'Company',\n",
    "           'Consumer consent provided?',\n",
    "           'Submitted via',\n",
    "           'Timely response?']\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "#building the column transformer with both transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "#fit the preprocessor, then transform trainging and test set, assign sparse matrix to variables\n",
    "preprocessor.fit(X)\n",
    "encX_train = preprocessor.transform(X_train)\n",
    "encX_test = preprocessor.transform(X_test)\n",
    "\n",
    "#Creating Dummy variable for target. Will be neccessary for DNN. \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "dummy_y_train = np_utils.to_categorical(encoder.transform(y_train))\n",
    "dummy_y_test = np_utils.to_categorical(encoder.transform(y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names_from_ColumnTransformer(column_transformer):    \n",
    "    col_name = []\n",
    "    for transformer_in_columns in column_transformer.transformers_[:-1]:#the last transformer is ColumnTransformer's 'remainder'\n",
    "        raw_col_name = transformer_in_columns[2]\n",
    "        if isinstance(transformer_in_columns[1],Pipeline): \n",
    "            transformer = transformer_in_columns[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = transformer_in_columns[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError: # if no 'get_feature_names' function, use raw column name\n",
    "            names = raw_col_name\n",
    "        if isinstance(names,np.ndarray): # eg.\n",
    "            col_name += names.tolist()\n",
    "        elif isinstance(names,list):\n",
    "            col_name += names    \n",
    "        elif isinstance(names,str):\n",
    "            col_name.append(names)\n",
    "    return col_name\n",
    "\n",
    "processedColumns = get_column_names_from_ColumnTransformer(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79930675494147\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                         Closed       0.00      0.00      0.00      5325\n",
      "        Closed with explanation       0.81      0.99      0.89    327800\n",
      "    Closed with monetary relief       0.51      0.02      0.03     23743\n",
      "Closed with non-monetary relief       0.52      0.07      0.13     53102\n",
      "             Closed with relief       0.48      0.11      0.18      1620\n",
      "          Closed without relief       0.76      0.93      0.83      5290\n",
      "\n",
      "                       accuracy                           0.80    416880\n",
      "                      macro avg       0.51      0.35      0.34    416880\n",
      "                   weighted avg       0.74      0.80      0.73    416880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1, solver='saga', penalty='l1', multi_class='multinomial')\n",
    "lr_para = {'C':[1.0,0.1,0.01], \n",
    "           'class_weight':[None,'balanced'],\n",
    "           'max_iter':[50,100,125]}\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(lr, lr_para,cv=5, scoring='accuracy', n_jobs=-1)\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting LogisiticRegression(), create prediciton from X_test data\n",
    "bestfitLR = fitmodel.best_estimator_\n",
    "\n",
    "bestfitLR.fit(encX_train,y_train)\n",
    "y_pred = bestfitLR.predict(encX_test)\n",
    "print(bestfitLR.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  72 | elapsed:   34.6s remaining:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  72 | elapsed:   36.2s remaining:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of  72 | elapsed:   40.8s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   41.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4593959892535022\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                         Closed       0.06      0.36      0.10      5325\n",
      "        Closed with explanation       0.87      0.45      0.59    327800\n",
      "    Closed with monetary relief       0.21      0.62      0.31     23743\n",
      "Closed with non-monetary relief       0.27      0.45      0.34     53102\n",
      "             Closed with relief       0.06      0.48      0.11      1620\n",
      "          Closed without relief       0.08      0.63      0.14      5290\n",
      "\n",
      "                       accuracy                           0.46    416880\n",
      "                      macro avg       0.26      0.50      0.26    416880\n",
      "                   weighted avg       0.74      0.46      0.53    416880\n",
      "\n",
      "BernoulliNB(alpha=10, binarize=0.1, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb_para = {'alpha':[1,2,10,0],\n",
    "            'fit_prior': [True,False],\n",
    "            'binarize': [0,0.1, 0.5]}\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(bnb, bnb_para,cv=3, scoring='accuracy', n_jobs=-1, verbose=10)\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting GradientBoostingClassifier(), create prediciton from X_test data\n",
    "bestfitBNB = fitmodel.best_estimator_\n",
    "\n",
    "bestfitBNB.fit(encX_train,y_train)\n",
    "y_pred = bestfitBNB.predict(encX_test)\n",
    "print(bestfitBNB.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(bestfitBNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, input_dim=128, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "972719/972719 [==============================] - 9030s 9ms/step - loss: 0.5500 - accuracy: 0.7998\n",
      "Epoch 2/10\n",
      "972719/972719 [==============================] - 8929s 9ms/step - loss: 0.5336 - accuracy: 0.8031\n",
      "Epoch 3/10\n",
      "972719/972719 [==============================] - 8906s 9ms/step - loss: 0.5309 - accuracy: 0.8039\n",
      "Epoch 4/10\n",
      "972719/972719 [==============================] - 8882s 9ms/step - loss: 0.5301 - accuracy: 0.8040\n",
      "Epoch 5/10\n",
      "972719/972719 [==============================] - 9010s 9ms/step - loss: 0.5297 - accuracy: 0.8042\n",
      "Epoch 6/10\n",
      "972719/972719 [==============================] - 9125s 9ms/step - loss: 0.5297 - accuracy: 0.8043\n",
      "Epoch 7/10\n",
      "972719/972719 [==============================] - 9054s 9ms/step - loss: 0.5293 - accuracy: 0.8044\n",
      "Epoch 8/10\n",
      "972719/972719 [==============================] - 9076s 9ms/step - loss: 0.5297 - accuracy: 0.8043\n",
      "Epoch 9/10\n",
      "972719/972719 [==============================] - 8957s 9ms/step - loss: 0.5302 - accuracy: 0.8044\n",
      "416880/416880 [==============================] - 1211s 3ms/step\n",
      "Accuracy: 80.38\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-826ceb06a5b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#see the classification report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy_y_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#Save the trained model into an .h5 file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[0;32m   1850\u001b[0m     \"\"\"\n\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and multiclass targets"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Build model with 2 FC layers each 50 nodes (relu activation), and 1 node output output layer (sigmoid activation)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, init='uniform', input_dim=128, activation='relu'))\n",
    "model.add(Dense(50, init='uniform', activation='relu'))\n",
    "model.add(Dense(6, init= 'uniform', activation = 'softmax'))\n",
    "\n",
    "#setup earlystop callback\n",
    "earlystop_callback = EarlyStopping(monitor='accuracy', min_delta=0.0001, patience=3)\n",
    "\n",
    "#Compiled with adam optimizer, binary crossentropy loss function, accuracy metric for evaluation\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#train the model over 100 epochs\n",
    "model.fit(encX_train, dummy_y_train, epochs=10, batch_size=10, callbacks=[earlystop_callback])\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, dummy_y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#Save the trained model into an .h5 file\n",
    "model.save(\"multiclass-model-50-50.h5\")\n",
    "\n",
    "#see the classification report\n",
    "#print(classification_report(dummy_y_test, model.predict_classes(encX_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the trained model into an .h5 file\n",
    "model.save(\"multiclass-model-50-50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.01      0.02      5243\n",
      "           1       0.81      0.99      0.89    327389\n",
      "           2       0.48      0.02      0.03     23767\n",
      "           3       0.63      0.11      0.19     53472\n",
      "           4       0.44      0.08      0.13      1574\n",
      "           5       0.77      0.90      0.83      5435\n",
      "\n",
      "   micro avg       0.80      0.80      0.80    416880\n",
      "   macro avg       0.64      0.35      0.35    416880\n",
      "weighted avg       0.76      0.80      0.74    416880\n",
      " samples avg       0.80      0.80      0.80    416880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "pred = model.predict(encX_test)\n",
    "\n",
    "pred_h1 = tf.one_hot(tf.argmax(pred, axis = 1), depth = 6)\n",
    "\n",
    "print(classification_report(dummy_y_test, pred_h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "pred_h1 = []\n",
    "\n",
    "for vect in pred:\n",
    "    pred_h1 = np.append(pred_h1,(vect == vect.max()).astype(int))\n",
    "    \n",
    "print(pred_h1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]], shape=(416880, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "pred_h1 = tf.one_hot(tf.argmax(pred, axis = 1), depth = 6)\n",
    "\n",
    "print(pred_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, vect in enumerate(pred):\n",
    "    assert (tf.argmax(vect) == tf.argmax(pred_h1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pred:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Resolution Classifier\n",
    "In the previous notebook \"Data Exploration\", the majority of data transformtion was done, and was copied and cut down into this first executable cell. To get more details on how and why transformations were done as below, please refer to the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the cell, the data will be ready to be pushed through various Machine Learning Classifying models. These will include Random Forest Classifier, Logisitic Regression, and Gradient Boosted Decision Tree Classifier. As a more experimentive aproach, SVC and SGDClassifier will also be trained after a SVD dimensionality-reduction. Lastly, Deep Neural Network (DNN) classifier models will be built and trained using a few different architechs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Defining what dtype to convert each column to\n",
    "#numberic columns are transformed after reading in\n",
    "dtype_dict = {'Product':\"category\",\n",
    "             'Consumer consent provided?': \"category\",\n",
    "             'Submitted via': \"category\",\n",
    "             'Company response to consumer': \"category\",\n",
    "             'Consumer disputed?': \"category\"}\n",
    "\n",
    "#read in .csv file, dates are parsed into datetime objects. \n",
    "#The Complaint ID is Unique in every entry, so it can be used as index\n",
    "df = pd.read_csv('Consumer_Complaints.csv',\n",
    "                 index_col=['Complaint ID'],\n",
    "                 parse_dates=[\"Date received\",\"Date sent to company\"],\n",
    "                 dtype=dtype_dict)\n",
    "\n",
    "#This will replace ending '-' to 5 (average linespace of 10)\n",
    "regexReplaceDash = r\"(\\d+)(-)$\"\n",
    "df['ZIP code'] = df['ZIP code'].str.replace(regexReplaceDash, r'\\g<1>5')\n",
    "\n",
    "#This will change ending XX to 50 (average linespace of 100)\n",
    "regex_XX = r'(\\d{3})(XX)'\n",
    "df['ZIP code'] = df['ZIP code'].str.replace(regex_XX, r'\\g<1>50')\n",
    "\n",
    "#This will remove all other entries that are still not 5 digits\n",
    "regexRemove = r'\\D+'\n",
    "df['ZIP code'] = df['ZIP code'].replace(regexRemove, np.nan, regex=True)\n",
    "\n",
    "#imputes the mean for nan \n",
    "imputeMean = df['ZIP code'].astype(np.float).mean()\n",
    "df['ZIP code'] = df['ZIP code'].astype(np.float).fillna(imputeMean)\n",
    "\n",
    "#Transforming 2 unique valued col to float boolean\n",
    "booleanize = {'Yes': 1, 'No': 0}\n",
    "df['Timely response?'] = pd.Series(df['Timely response?'].map(booleanize), dtype = np.float)\n",
    "\n",
    "#function to apply to column to convert less common results to 'Other', as well as NaN\n",
    "def convertToOther(value, keepList):\n",
    "    if (value == ''):\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        return value if value in keepList else \"Other\"\n",
    "    \n",
    "#Lists top 23 value counts (allowed to exclude values), turns NaN to '' to others, converts to category dtype\n",
    "def cleanReduceConvert(df, column, blackList=[]):\n",
    "    keepList = []\n",
    "    for category in df[column].value_counts().head(23).index.tolist():\n",
    "        if (category.lower().split()[0] != \"other\"):\n",
    "            keepList.append(category)\n",
    "    for category in blackList:\n",
    "        try:\n",
    "            keepList.remove(category)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    df[column].fillna('', inplace=True)\n",
    "    return pd.Series(df[column].apply(convertToOther, args=(keepList,)), dtype = 'category')\n",
    "\n",
    "df['Sub-product'] = cleanReduceConvert(df, 'Sub-product', blackList= ['I do not know'])\n",
    "df['Issue'] = cleanReduceConvert(df, 'Issue')\n",
    "df['Sub-issue'] = cleanReduceConvert(df, 'Sub-issue')\n",
    "df['Company'] = cleanReduceConvert(df, 'Company')\n",
    "\n",
    "def entryOrNull(strVal):\n",
    "    return 1.0 if strVal is not np.nan else 0.0\n",
    "\n",
    "df['Consumer complaint narrative submitted?'] = df['Consumer complaint narrative'].apply(entryOrNull)\n",
    "\n",
    "def dtToCols(df, dtcolumn):\n",
    "    df[\"{} day\".format(dtcolumn)] = df[dtcolumn].dt.day\n",
    "    df[\"{} month\".format(dtcolumn)] = df[dtcolumn].dt.month\n",
    "    df[\"{} year\".format(dtcolumn)] = df[dtcolumn].dt.year\n",
    "    \n",
    "dtToCols(df, \"Date received\")\n",
    "dtToCols(df, \"Date sent to company\")\n",
    "\n",
    "df[\"Consumer consent provided?\"] = df[\"Consumer consent provided?\"].cat.add_categories(\"Not recorded\").fillna(\"Not recorded\")\n",
    "\n",
    "df = df.drop(df[df[\"Company response to consumer\"].isna()].index)\n",
    "\n",
    "dfInProgress = df[df[\"Company response to consumer\"] == \"In progress\"]\n",
    "df = df[df[\"Company response to consumer\"] != \"In progress\"]\n",
    "\n",
    "dfUntimelyResponse = df[df[\"Company response to consumer\"] == \"Untimely response\"]\n",
    "df = df[df[\"Company response to consumer\"] != \"Untimely response\"]\n",
    "\n",
    "twoOutputsDict = {\"Closed with explanation\":\"Closed without relief\", \n",
    "                  \"Closed with non-monetary relief\":\"Closed with relief\",\n",
    "                  \"Closed with monetary relief\":\"Closed with relief\",\n",
    "                  \"Closed without relief\":\"Closed without relief\", \n",
    "                  \"Closed\":\"Closed without relief\",\n",
    "                  \"Closed with relief\":\"Closed with relief\"}\n",
    "df[\"Company response to consumer\"] = df[\"Company response to consumer\"].map(twoOutputsDict)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#data columns not be used for the model\n",
    "dropList = [\"Consumer complaint narrative\",\n",
    "            \"Company public response\",\n",
    "            \"State\",\n",
    "            \"Tags\",\n",
    "            \"Consumer disputed?\",\n",
    "            \"Date received\", \n",
    "            \"Date sent to company\",\n",
    "            \"Company response to consumer\"]\n",
    "X = df.drop(dropList, axis=1)\n",
    "Y = df[\"Company response to consumer\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "#Columns to be standard scaled/imputed\n",
    "numeric_features = ['ZIP code',\n",
    "                    'Date received day',\n",
    "                    'Date received month',\n",
    "                    'Date received year',\n",
    "                    'Date sent to company day',\n",
    "                    'Date sent to company month',\n",
    "                    'Date sent to company year']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "#Columns to one hot encoded\n",
    "categorical_features = ['Product',\n",
    "           'Sub-product',\n",
    "           'Issue',\n",
    "           'Sub-issue',\n",
    "           'Company',\n",
    "           'Consumer consent provided?',\n",
    "           'Submitted via',\n",
    "           'Timely response?']\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "#building the column transformer with both transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "#fit the preprocessor, then transform trainging and test set, assign sparse matrix to variables\n",
    "preprocessor.fit(X)\n",
    "encX_train = preprocessor.transform(X_train)\n",
    "encX_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are multiple transformers wrapped in the ColumnTransformer, this function below provided by pjgao creates a list of the columns. This is useful for feature importances after running the Model Trainging. Heres a link to where the code was found: https://github.com/scikit-learn/scikit-learn/issues/12525#issuecomment-436217100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names_from_ColumnTransformer(column_transformer):    \n",
    "    col_name = []\n",
    "    for transformer_in_columns in column_transformer.transformers_[:-1]:#the last transformer is ColumnTransformer's 'remainder'\n",
    "        raw_col_name = transformer_in_columns[2]\n",
    "        if isinstance(transformer_in_columns[1],Pipeline): \n",
    "            transformer = transformer_in_columns[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = transformer_in_columns[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError: # if no 'get_feature_names' function, use raw column name\n",
    "            names = raw_col_name\n",
    "        if isinstance(names,np.ndarray): # eg.\n",
    "            col_name += names.tolist()\n",
    "        elif isinstance(names,list):\n",
    "            col_name += names    \n",
    "        elif isinstance(names,str):\n",
    "            col_name.append(names)\n",
    "    return col_name\n",
    "\n",
    "processedColumns = get_column_names_from_ColumnTransformer(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier\n",
    "The first model to be run on this will be RandomForestClassifier from sklearn's library. A grid search is used to find the better tuning of the hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8117695739781232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Closed with relief       0.00      0.00      0.00     52313\n",
      "Closed without relief       0.81      1.00      0.90    225607\n",
      "\n",
      "            micro avg       0.81      0.81      0.81    277920\n",
      "            macro avg       0.41      0.50      0.45    277920\n",
      "         weighted avg       0.66      0.81      0.73    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#define classifier and parameter grid\n",
    "rf = RandomForestClassifier()\n",
    "forest_para = {'criterion':['gini','entropy'],\n",
    "                'min_samples_split' : range(2,6,2),\n",
    "                'max_depth': range(3,7,2),\n",
    "                'n_estimators':range(10,30,10)}\n",
    "\n",
    "\n",
    "#Grid search to find best hyper parameters\n",
    "fitmodel = GridSearchCV(rf,forest_para,cv=3, scoring='roc_auc')\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting model, create prediciton from X_test data\n",
    "bestfitRF = fitmodel.best_estimator_\n",
    "\n",
    "#fit once more on the best estimator\n",
    "bestfitRF.fit(encX_train,y_train)\n",
    "y_pred = bestfitRF.predict(encX_test)\n",
    "print(bestfitRF.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the random forrest couldn't make any prediction with this dataset, as it went the lazy underfitting route of gussing every label to be the majority class. This is likely due to the One Hot Encoding with too many categories. Since the tree makes a few set of decisions, and the max depth goes to 7, there's not much predictive power in only using 7 columns in a row that are all 0 (for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression\n",
    "Next, the data will be run through a LogisticRegression Model. Note that 'l1' penalty was used because that penalty usually zeros out less important coefficients, which since this was one hot encoded, is expected to happen a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6753598157743236\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Closed with relief       0.32      0.66      0.43     52229\n",
      "Closed without relief       0.90      0.68      0.77    225691\n",
      "\n",
      "            micro avg       0.68      0.68      0.68    277920\n",
      "            macro avg       0.61      0.67      0.60    277920\n",
      "         weighted avg       0.79      0.68      0.71    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1, solver='saga', penalty='l1')\n",
    "lr_para = {'C':[1.0,0.1,0.01], \n",
    "           'class_weight':[None,'balanced'],\n",
    "           'max_iter':[50,100,150]}\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(lr, lr_para,cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting LogisiticRegression(), create prediciton from X_test data\n",
    "bestfitLR = fitmodel.best_estimator_\n",
    "\n",
    "bestfitLR.fit(encX_train,y_train)\n",
    "y_pred = bestfitLR.predict(encX_test)\n",
    "print(bestfitLR.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the score was low for this, looking at precision, recall, and f1, it seems like a improvement from guessing the majority class everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=-1, penalty='l1', random_state=None,\n",
      "          solver='saga', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>x0_Prepaid card</td>\n",
       "      <td>-1.31532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>x3_Personal information incorrect</td>\n",
       "      <td>-1.24942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>x4_Experian Information Solutions Inc.</td>\n",
       "      <td>-1.06433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>x4_PORTFOLIO RECOVERY ASSOCIATES INC</td>\n",
       "      <td>-0.839405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>x4_ENCORE CAPITAL GROUP INC.</td>\n",
       "      <td>-0.737273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>x0_Credit card or prepaid card</td>\n",
       "      <td>-0.719218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>x4_AES/PHEAA</td>\n",
       "      <td>-0.645815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>x3_Information belongs to someone else</td>\n",
       "      <td>-0.567322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>x4_TD BANK US HOLDING COMPANY</td>\n",
       "      <td>-0.547387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>x1_Checking account</td>\n",
       "      <td>-0.519221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>x0_Payday loan</td>\n",
       "      <td>0.726836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>x3_Reporting company used your report improperly</td>\n",
       "      <td>0.743417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>x2_Struggling to pay mortgage</td>\n",
       "      <td>0.800414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>x3_Account status</td>\n",
       "      <td>0.81214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>x3_Account terms</td>\n",
       "      <td>0.884882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>x4_Navient Solutions, LLC.</td>\n",
       "      <td>0.929411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>x4_HSBC NORTH AMERICA HOLDINGS INC.</td>\n",
       "      <td>1.51375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>x4_OCWEN FINANCIAL CORPORATION</td>\n",
       "      <td>1.67055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>x0_Virtual currency</td>\n",
       "      <td>1.94178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>x4_NATIONSTAR MORTGAGE</td>\n",
       "      <td>2.54764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0         1\n",
       "21                                    x0_Prepaid card  -1.31532\n",
       "86                  x3_Personal information incorrect  -1.24942\n",
       "100            x4_Experian Information Solutions Inc.  -1.06433\n",
       "108              x4_PORTFOLIO RECOVERY ASSOCIATES INC -0.839405\n",
       "98                       x4_ENCORE CAPITAL GROUP INC. -0.737273\n",
       "11                     x0_Credit card or prepaid card -0.719218\n",
       "91                                       x4_AES/PHEAA -0.645815\n",
       "81             x3_Information belongs to someone else -0.567322\n",
       "111                     x4_TD BANK US HOLDING COMPANY -0.547387\n",
       "25                                x1_Checking account -0.519221\n",
       "19                                     x0_Payday loan  0.726836\n",
       "89   x3_Reporting company used your report improperly  0.743417\n",
       "64                      x2_Struggling to pay mortgage  0.800414\n",
       "68                                  x3_Account status   0.81214\n",
       "70                                   x3_Account terms  0.884882\n",
       "104                        x4_Navient Solutions, LLC.  0.929411\n",
       "101               x4_HSBC NORTH AMERICA HOLDINGS INC.   1.51375\n",
       "105                    x4_OCWEN FINANCIAL CORPORATION   1.67055\n",
       "24                                x0_Virtual currency   1.94178\n",
       "103                            x4_NATIONSTAR MORTGAGE   2.54764"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bestfitLR)\n",
    "\n",
    "coef_df = pd.DataFrame([processedColumns,list(bestfitLR.coef_[0])]).transpose().sort_values(1)\n",
    "coef_df.iloc[np.r_[0:10, -10:0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, it seems like a big influence is what company the complaint is being filled out for, the sub-issue, and what type of product the complaint pertains to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier\n",
    "Next the a gradient boosted tree classifier will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8208945020149684\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Closed with relief       0.64      0.11      0.18     52229\n",
      "Closed without relief       0.83      0.99      0.90    225691\n",
      "\n",
      "            micro avg       0.82      0.82      0.82    277920\n",
      "            macro avg       0.73      0.55      0.54    277920\n",
      "         weighted avg       0.79      0.82      0.77    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_para = {'loss':['deviance', 'exponential'],\n",
    "            'learning_rate': [0.1, 0.05],\n",
    "            'max_depth': [3,7]}\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(gbc, gbc_para,cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting GradientBoostingClassifier(), create prediciton from X_test data\n",
    "bestfitGBC = fitmodel.best_estimator_\n",
    "\n",
    "bestfitGBC.fit(encX_train,y_train)\n",
    "y_pred = bestfitGBC.predict(encX_test)\n",
    "print(bestfitGBC.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='exponential', max_depth=7,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>x4_Experian Information Solutions Inc.</td>\n",
       "      <td>0.204651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Date sent to company year</td>\n",
       "      <td>0.0825464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>x0_Credit card</td>\n",
       "      <td>0.0761093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>x1_Checking account</td>\n",
       "      <td>0.0633851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>x0_Mortgage</td>\n",
       "      <td>0.0338467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>x0_Credit reporting</td>\n",
       "      <td>0.0319135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZIP code</td>\n",
       "      <td>0.0314691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>x0_Credit card or prepaid card</td>\n",
       "      <td>0.0284411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Date sent to company month</td>\n",
       "      <td>0.0257309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>x4_Other</td>\n",
       "      <td>0.0251874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>x2_False statements or representation</td>\n",
       "      <td>6.43743e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>x3_Didn't receive enough information to verify...</td>\n",
       "      <td>6.02636e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>x0_Payday loan, title loan, or personal loan</td>\n",
       "      <td>5.86243e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>x2_Attempts to collect debt not owed</td>\n",
       "      <td>5.83568e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>x3_Old information reappears or never goes away</td>\n",
       "      <td>5.37255e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>x2_Deposits and withdrawals</td>\n",
       "      <td>4.70923e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>x3_Public record</td>\n",
       "      <td>3.14281e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>x0_Other financial service</td>\n",
       "      <td>3.00637e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>x5_Consent withdrawn</td>\n",
       "      <td>1.47889e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>x0_Virtual currency</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0            1\n",
       "100             x4_Experian Information Solutions Inc.     0.204651\n",
       "6                            Date sent to company year    0.0825464\n",
       "10                                      x0_Credit card    0.0761093\n",
       "25                                 x1_Checking account    0.0633851\n",
       "17                                         x0_Mortgage    0.0338467\n",
       "12                                 x0_Credit reporting    0.0319135\n",
       "0                                             ZIP code    0.0314691\n",
       "11                      x0_Credit card or prepaid card    0.0284411\n",
       "5                           Date sent to company month    0.0257309\n",
       "106                                           x4_Other    0.0251874\n",
       "54               x2_False statements or representation  6.43743e-05\n",
       "79   x3_Didn't receive enough information to verify...  6.02636e-05\n",
       "20        x0_Payday loan, title loan, or personal loan  5.86243e-05\n",
       "46                x2_Attempts to collect debt not owed  5.83568e-05\n",
       "84     x3_Old information reappears or never goes away  5.37255e-05\n",
       "52                         x2_Deposits and withdrawals  4.70923e-05\n",
       "87                                    x3_Public record  3.14281e-05\n",
       "18                          x0_Other financial service  3.00637e-05\n",
       "117                               x5_Consent withdrawn  1.47889e-05\n",
       "24                                 x0_Virtual currency            0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bestfitGBC)\n",
    "\n",
    "coef_df = pd.DataFrame([processedColumns,list(bestfitGBC.feature_importances_)]).transpose().sort_values(1, ascending=False)\n",
    "coef_df.iloc[np.r_[0:10, -10:0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears here that the biggest influence in prediction is the type of products. There also seems to be an issue with Experian Information Solutions Inc. being consitent with their response, what ever it may be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "Since there was an imbalance of classes, I've resample by downsampling the majority class to create a 1:1 ratio between the classes, to see if this improves predicting power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "#seperate the target based on classification\n",
    "df_resample = pd.concat([X_train, y_train],axis=1)\n",
    "not_relief = df_resample[df_resample['Company response to consumer']=='Closed without relief']\n",
    "relief = df_resample[df_resample['Company response to consumer']=='Closed with relief']\n",
    "\n",
    "#resample the majority class with equal frequency as minorty class\n",
    "not_relief_downsample = resample(not_relief,\n",
    "                                 replace=False,\n",
    "                                 n_samples = len(relief))\n",
    "\n",
    "#concat resampled majority class with minorty class\n",
    "downsample = pd.concat([not_relief_downsample, relief])\n",
    "\n",
    "X_train = df.drop(dropList, axis=1)\n",
    "y_train = df[\"Company response to consumer\"]\n",
    "\n",
    "#fit the preprocessor, then transform trainging and test set, assign sparse matrix to variables\n",
    "encX_train = preprocessor.transform(X_train)\n",
    "encX_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6755001439263097\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Closed with relief       0.32      0.66      0.43     52452\n",
      "Closed without relief       0.90      0.68      0.77    225468\n",
      "\n",
      "             accuracy                           0.68    277920\n",
      "            macro avg       0.61      0.67      0.60    277920\n",
      "         weighted avg       0.79      0.68      0.71    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "lr_para = {'C':[1.0,0.1,0.01], \n",
    "           'solver':['liblinear','saga'],\n",
    "           'class_weight':[None,'balanced'],\n",
    "           'max_iter':[50,100,150]}\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(lr, lr_para,cv=5, scoring='roc_auc')\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting LogisticRegression(), create prediciton from X_test data\n",
    "bestfitLR = fitmodel.best_estimator_\n",
    "\n",
    "bestfitLR.fit(encX_train,y_train)\n",
    "y_pred = bestfitLR.predict(encX_test)\n",
    "print(bestfitLR.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be any significant improvement for the logistic regression after resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=50, multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>x0_Prepaid card</td>\n",
       "      <td>-1.18221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>x3_Personal information incorrect</td>\n",
       "      <td>-1.16044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>x4_Experian Information Solutions Inc.</td>\n",
       "      <td>-1.07654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>x4_PORTFOLIO RECOVERY ASSOCIATES INC</td>\n",
       "      <td>-0.812528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>x4_ENCORE CAPITAL GROUP INC.</td>\n",
       "      <td>-0.716444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>x0_Credit card or prepaid card</td>\n",
       "      <td>-0.654846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>x3_Information belongs to someone else</td>\n",
       "      <td>-0.553695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>x4_AES/PHEAA</td>\n",
       "      <td>-0.553528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>x4_TD BANK US HOLDING COMPANY</td>\n",
       "      <td>-0.523632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>x1_Checking account</td>\n",
       "      <td>-0.521711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>x3_Account status</td>\n",
       "      <td>0.657722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>x2_Credit reporting company's investigation</td>\n",
       "      <td>0.67033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>x4_PNC Bank N.A.</td>\n",
       "      <td>0.671164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>x3_Account terms</td>\n",
       "      <td>0.704198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>x2_Struggling to pay mortgage</td>\n",
       "      <td>0.766549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>x3_Reporting company used your report improperly</td>\n",
       "      <td>0.891666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>x4_Navient Solutions, LLC.</td>\n",
       "      <td>0.934995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>x4_HSBC NORTH AMERICA HOLDINGS INC.</td>\n",
       "      <td>1.41488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>x4_OCWEN FINANCIAL CORPORATION</td>\n",
       "      <td>1.5841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>x4_NATIONSTAR MORTGAGE</td>\n",
       "      <td>2.35095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0         1\n",
       "21                                    x0_Prepaid card  -1.18221\n",
       "86                  x3_Personal information incorrect  -1.16044\n",
       "100            x4_Experian Information Solutions Inc.  -1.07654\n",
       "108              x4_PORTFOLIO RECOVERY ASSOCIATES INC -0.812528\n",
       "98                       x4_ENCORE CAPITAL GROUP INC. -0.716444\n",
       "11                     x0_Credit card or prepaid card -0.654846\n",
       "81             x3_Information belongs to someone else -0.553695\n",
       "91                                       x4_AES/PHEAA -0.553528\n",
       "111                     x4_TD BANK US HOLDING COMPANY -0.523632\n",
       "25                                x1_Checking account -0.521711\n",
       "68                                  x3_Account status  0.657722\n",
       "50        x2_Credit reporting company's investigation   0.67033\n",
       "107                                  x4_PNC Bank N.A.  0.671164\n",
       "70                                   x3_Account terms  0.704198\n",
       "64                      x2_Struggling to pay mortgage  0.766549\n",
       "89   x3_Reporting company used your report improperly  0.891666\n",
       "104                        x4_Navient Solutions, LLC.  0.934995\n",
       "101               x4_HSBC NORTH AMERICA HOLDINGS INC.   1.41488\n",
       "105                    x4_OCWEN FINANCIAL CORPORATION    1.5841\n",
       "103                            x4_NATIONSTAR MORTGAGE   2.35095"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bestfitLR)\n",
    "\n",
    "coef_df = pd.DataFrame([processedColumns,list(bestfitLR.coef_[0])]).transpose().sort_values(1)\n",
    "coef_df.iloc[np.r_[0:10, -10:0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, it is observed that resampling did not significantly change the feature importance of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8149431491076569\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Closed with relief       0.67      0.04      0.07     52452\n",
      "Closed without relief       0.82      1.00      0.90    225468\n",
      "\n",
      "             accuracy                           0.81    277920\n",
      "            macro avg       0.74      0.52      0.48    277920\n",
      "         weighted avg       0.79      0.81      0.74    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_para = {'loss':['deviance', 'exponential'],\n",
    "            'learning_rate': [0.1, 0.05],\n",
    "            'max_depth': [3,7]}\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(gbc, gbc_para,cv=3, scoring='roc_auc')\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting GradientBoostingClassifier(), create prediciton from X_test data\n",
    "bestfitGBC = fitmodel.best_estimator_\n",
    "\n",
    "bestfitGBC.fit(encX_train,y_train)\n",
    "y_pred = bestfitGBC.predict(encX_test)\n",
    "print(bestfitGBC.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the resampling made Gradient Boosted Classifier preform slightly worse than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.05, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>x4_Experian Information Solutions Inc.</td>\n",
       "      <td>0.305674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>x0_Mortgage</td>\n",
       "      <td>0.0891856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>x0_Credit card</td>\n",
       "      <td>0.0872126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>x1_Checking account</td>\n",
       "      <td>0.0815731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Date sent to company year</td>\n",
       "      <td>0.0620886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>x4_Other</td>\n",
       "      <td>0.0525954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>x4_BANK OF AMERICA, NATIONAL ASSOCIATION</td>\n",
       "      <td>0.0253743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>x0_Credit card or prepaid card</td>\n",
       "      <td>0.0243908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>x4_TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
       "      <td>0.0238843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>x3_Information belongs to someone else</td>\n",
       "      <td>0.0232868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>x2_Trouble during payment process</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>x2_Managing the loan or lease</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>x2_Loan servicing, payments, escrow account</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>x2_Incorrect information on your report</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>x2_Incorrect information on credit report</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>x2_Improper use of your report</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>x2_False statements or representation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>x2_Disclosure verification of debt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>x2_Deposits and withdrawals</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>x7_1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0          1\n",
       "100       x4_Experian Information Solutions Inc.   0.305674\n",
       "17                                   x0_Mortgage  0.0891856\n",
       "10                                x0_Credit card  0.0872126\n",
       "25                           x1_Checking account  0.0815731\n",
       "6                      Date sent to company year  0.0620886\n",
       "106                                     x4_Other  0.0525954\n",
       "93      x4_BANK OF AMERICA, NATIONAL ASSOCIATION  0.0253743\n",
       "11                x0_Credit card or prepaid card  0.0243908\n",
       "112    x4_TRANSUNION INTERMEDIATE HOLDINGS, INC.  0.0238843\n",
       "81        x3_Information belongs to someone else  0.0232868\n",
       "65             x2_Trouble during payment process          0\n",
       "61                 x2_Managing the loan or lease          0\n",
       "59   x2_Loan servicing, payments, escrow account          0\n",
       "57       x2_Incorrect information on your report          0\n",
       "56     x2_Incorrect information on credit report          0\n",
       "55                x2_Improper use of your report          0\n",
       "54         x2_False statements or representation          0\n",
       "53            x2_Disclosure verification of debt          0\n",
       "52                   x2_Deposits and withdrawals          0\n",
       "127                                       x7_1.0          0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bestfitGBC)\n",
    "\n",
    "coef_df = pd.DataFrame([processedColumns,list(bestfitGBC.feature_importances_)]).transpose().sort_values(1, ascending=False)\n",
    "coef_df.iloc[np.r_[0:10, -10:0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the Logistic Regression, resampling did not significantly affect the feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Singular Value Decomposition(SVD)\n",
    "\n",
    "Below, there was an attempt to run a Support Vector Machine Approach, along with a TruncatedSVD demensionality reduction. However, the training yielded weak results, and the SVC was set to take weeks to train, for questionable returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#fit the demensionality reducer to 2 dimensions\n",
    "svd = TruncatedSVD()\n",
    "svd.fit(preprocessor.transform(X))\n",
    "\n",
    "#transfrom input\n",
    "encX_train = svd.transform(preprocessor.transform(X_train))\n",
    "encX_test = svd.transform(preprocessor.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] C=1, class_weight=None, degree=2, gamma=0.001 ...................\n",
      "[CV]  C=1, class_weight=None, degree=2, gamma=0.001, score=0.529, total=1133.4min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 1133.4min remaining:    0.0s\n",
      "[CV] C=1, class_weight=None, degree=2, gamma=0.001 ...................\n",
      "[CV]  C=1, class_weight=None, degree=2, gamma=0.001, score=0.568, total=993.3min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 2126.7min remaining:    0.0s\n",
      "[CV] C=1, class_weight=None, degree=2, gamma=0.001 ...................\n",
      "[CV]  C=1, class_weight=None, degree=2, gamma=0.001, score=0.468, total=1096.0min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 3222.7min remaining:    0.0s\n",
      "[CV] C=1, class_weight=None, degree=2, gamma=1 .......................\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svc = SVC(probability=True, kernel='poly')\n",
    "svc_para = {'C':[1, 0.01],\n",
    "            'gamma': [0.001,1],\n",
    "            'class_weight':[None, 'balanced'],\n",
    "            'degree': [2,3]}\n",
    "\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(svc, svc_para,cv=3, scoring='roc_auc', verbose=100)\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting SVC(), create prediciton from X_test data\n",
    "bestfitSVC = fitmodel.best_estimator_\n",
    "\n",
    "bestfitSVC.fit(encX_train,y_train)\n",
    "y_pred = bestfitSVC.predict(encX_test)\n",
    "print(bestfitSVC.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine with Stocatic Gradient Descent\n",
    "(Incomplete investigation due to long training time, and unpromising returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done  59 out of  81 | elapsed:   27.7s remaining:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  81 | elapsed:   32.8s remaining:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  77 out of  81 | elapsed:  3.1min remaining:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8111327000575705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Closed with relief       0.00      0.00      0.00     52490\n",
      "Closed without relief       0.81      1.00      0.90    225430\n",
      "\n",
      "             accuracy                           0.81    277920\n",
      "            macro avg       0.41      0.50      0.45    277920\n",
      "         weighted avg       0.66      0.81      0.73    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sgdc = SGDClassifier(penalty='elasticnet')\n",
    "sgdc_para = {'loss':['hinge','log','squared_hinge'],\n",
    "            'l1_ratio':[0,.5,1], \n",
    "            'alpha':[0.0001, 0.01,1]}\n",
    "\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(sgdc, sgdc_para,cv=3, n_jobs=-1, verbose=10)\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting SGDClassifier(), create prediciton from X_test data\n",
    "bestfitSGDC = fitmodel.best_estimator_\n",
    "\n",
    "bestfitSGDC.fit(encX_train,y_train)\n",
    "y_pred = bestfitSGDC.predict(encX_test)\n",
    "print(bestfitSGDC.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network (DNN) Classifier\n",
    "\n",
    "Below, the target is turned into a boolean 0-1 column, so that it can be processed in a deep neural network that will be built on Keras with Tensorflow and a backend. The architecture used was 128 input, to 50 Fully Connected (FC) layer, to a second FC layer, both with relu activation, to a final output layer, with sigmoid activation. An adam optimizer was used with binary crossentropy as the loss function, and accuracy as the metric. The model was trained on 100 epochs. Models after this one only differ by number of layers and layer size.\n",
    "\n",
    "Note that in the classification reports, \"1\" is \"closed with relief\" and thus the recall and f1 we are more interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "booly_train = y_train.apply(lambda x: 1 if x=='Closed with relief' else 0)\n",
    "booly_test = y_test.apply(lambda x: 1 if x=='Closed with relief' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the error that appears in the cell occurs after training. Since training took about 12 hours, I found it better to correct and cell and after wards without rerunning it, and posting the resutls in the next cell. The cell should still run correctly if ran again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Hidden Layers: 128->50->50->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, input_dim=128, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "1111679/1111679 [==============================] - 574s 516us/step - loss: 0.4246 - accuracy: 0.8165\n",
      "Epoch 2/100\n",
      "1111679/1111679 [==============================] - 570s 513us/step - loss: 0.4178 - accuracy: 0.8184\n",
      "Epoch 3/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4161 - accuracy: 0.8190\n",
      "Epoch 4/100\n",
      "1111679/1111679 [==============================] - 572s 514us/step - loss: 0.4152 - accuracy: 0.8194\n",
      "Epoch 5/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4145 - accuracy: 0.8197\n",
      "Epoch 6/100\n",
      "1111679/1111679 [==============================] - 573s 515us/step - loss: 0.4140 - accuracy: 0.8199\n",
      "Epoch 7/100\n",
      "1111679/1111679 [==============================] - 574s 517us/step - loss: 0.4137 - accuracy: 0.8201\n",
      "Epoch 8/100\n",
      "1111679/1111679 [==============================] - 571s 513us/step - loss: 0.4135 - accuracy: 0.8202\n",
      "Epoch 9/100\n",
      "1111679/1111679 [==============================] - 571s 513us/step - loss: 0.4134 - accuracy: 0.8204\n",
      "Epoch 10/100\n",
      "1111679/1111679 [==============================] - 570s 513us/step - loss: 0.4133 - accuracy: 0.8203\n",
      "Epoch 11/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4131 - accuracy: 0.8203\n",
      "Epoch 12/100\n",
      "1111679/1111679 [==============================] - 572s 514us/step - loss: 0.4132 - accuracy: 0.8203\n",
      "Epoch 13/100\n",
      "1111679/1111679 [==============================] - 573s 516us/step - loss: 0.4131 - accuracy: 0.8205\n",
      "Epoch 14/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4130 - accuracy: 0.8204\n",
      "Epoch 15/100\n",
      "1111679/1111679 [==============================] - 584s 525us/step - loss: 0.4129 - accuracy: 0.8203\n",
      "Epoch 16/100\n",
      "1111679/1111679 [==============================] - 570s 513us/step - loss: 0.4130 - accuracy: 0.8204\n",
      "Epoch 17/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4131 - accuracy: 0.8204\n",
      "Epoch 18/100\n",
      "1111679/1111679 [==============================] - 574s 516us/step - loss: 0.4130 - accuracy: 0.8204\n",
      "Epoch 19/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4132 - accuracy: 0.8204\n",
      "Epoch 20/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4131 - accuracy: 0.8205\n",
      "Epoch 21/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4131 - accuracy: 0.8204\n",
      "Epoch 22/100\n",
      "1111679/1111679 [==============================] - 571s 513us/step - loss: 0.4133 - accuracy: 0.8205\n",
      "Epoch 23/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4133 - accuracy: 0.8205\n",
      "Epoch 24/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4133 - accuracy: 0.8205\n",
      "Epoch 25/100\n",
      "1111679/1111679 [==============================] - 572s 514us/step - loss: 0.4133 - accuracy: 0.8206\n",
      "Epoch 26/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4132 - accuracy: 0.8205\n",
      "Epoch 27/100\n",
      "1111679/1111679 [==============================] - 571s 513us/step - loss: 0.4133 - accuracy: 0.8204\n",
      "Epoch 28/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4134 - accuracy: 0.8205\n",
      "Epoch 29/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4135 - accuracy: 0.8205\n",
      "Epoch 30/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4135 - accuracy: 0.8204\n",
      "Epoch 31/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4135 - accuracy: 0.8204\n",
      "Epoch 32/100\n",
      "1111679/1111679 [==============================] - 572s 514us/step - loss: 0.4137 - accuracy: 0.8202\n",
      "Epoch 33/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4136 - accuracy: 0.8205\n",
      "Epoch 34/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4137 - accuracy: 0.8204\n",
      "Epoch 35/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4138 - accuracy: 0.8203\n",
      "Epoch 36/100\n",
      "1111679/1111679 [==============================] - 573s 515us/step - loss: 0.4137 - accuracy: 0.8203\n",
      "Epoch 37/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4139 - accuracy: 0.8202\n",
      "Epoch 38/100\n",
      "1111679/1111679 [==============================] - 573s 515us/step - loss: 0.4141 - accuracy: 0.8201\n",
      "Epoch 39/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4141 - accuracy: 0.8204\n",
      "Epoch 40/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4141 - accuracy: 0.8201\n",
      "Epoch 41/100\n",
      "1111679/1111679 [==============================] - 573s 516us/step - loss: 0.4142 - accuracy: 0.8202\n",
      "Epoch 42/100\n",
      "1111679/1111679 [==============================] - 573s 515us/step - loss: 0.4144 - accuracy: 0.8202\n",
      "Epoch 43/100\n",
      "1111679/1111679 [==============================] - 572s 514us/step - loss: 0.4143 - accuracy: 0.8202\n",
      "Epoch 44/100\n",
      "1111679/1111679 [==============================] - 573s 516us/step - loss: 0.4146 - accuracy: 0.8203\n",
      "Epoch 45/100\n",
      "1111679/1111679 [==============================] - 573s 515us/step - loss: 0.4145 - accuracy: 0.8203\n",
      "Epoch 46/100\n",
      "1111679/1111679 [==============================] - 572s 514us/step - loss: 0.4145 - accuracy: 0.8203\n",
      "Epoch 47/100\n",
      "1111679/1111679 [==============================] - 572s 514us/step - loss: 0.4145 - accuracy: 0.8204\n",
      "Epoch 48/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4145 - accuracy: 0.8203\n",
      "Epoch 49/100\n",
      "1111679/1111679 [==============================] - 572s 514us/step - loss: 0.4148 - accuracy: 0.8203\n",
      "Epoch 50/100\n",
      "1111679/1111679 [==============================] - 573s 516us/step - loss: 0.4147 - accuracy: 0.8203\n",
      "Epoch 51/100\n",
      "1111679/1111679 [==============================] - 573s 515us/step - loss: 0.4147 - accuracy: 0.8203\n",
      "Epoch 52/100\n",
      "1111679/1111679 [==============================] - 572s 514us/step - loss: 0.4149 - accuracy: 0.8203\n",
      "Epoch 53/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4150 - accuracy: 0.8204\n",
      "Epoch 54/100\n",
      "1111679/1111679 [==============================] - 572s 515us/step - loss: 0.4151 - accuracy: 0.8204\n",
      "Epoch 55/100\n",
      "1111679/1111679 [==============================] - 573s 515us/step - loss: 0.4150 - accuracy: 0.8201\n",
      "Epoch 56/100\n",
      "1111679/1111679 [==============================] - 573s 515us/step - loss: 0.4153 - accuracy: 0.8203\n",
      "Epoch 57/100\n",
      "1111679/1111679 [==============================] - 573s 515us/step - loss: 0.4155 - accuracy: 0.8201\n",
      "Epoch 58/100\n",
      "1111679/1111679 [==============================] - 573s 516us/step - loss: 0.4156 - accuracy: 0.8201\n",
      "Epoch 59/100\n",
      "1111679/1111679 [==============================] - 571s 514us/step - loss: 0.4154 - accuracy: 0.8201\n",
      "Epoch 60/100\n",
      "1111679/1111679 [==============================] - 580s 522us/step - loss: 0.4155 - accuracy: 0.8201\n",
      "Epoch 61/100\n",
      "1111679/1111679 [==============================] - 610s 548us/step - loss: 0.4156 - accuracy: 0.8202\n",
      "Epoch 62/100\n",
      "1111679/1111679 [==============================] - 608s 547us/step - loss: 0.4158 - accuracy: 0.8201\n",
      "Epoch 63/100\n",
      "1111679/1111679 [==============================] - 611s 549us/step - loss: 0.4159 - accuracy: 0.8199\n",
      "Epoch 64/100\n",
      "1111679/1111679 [==============================] - 613s 552us/step - loss: 0.4160 - accuracy: 0.8201\n",
      "Epoch 65/100\n",
      "1111679/1111679 [==============================] - 611s 550us/step - loss: 0.4162 - accuracy: 0.8201\n",
      "Epoch 66/100\n",
      "1111679/1111679 [==============================] - 614s 552us/step - loss: 0.4160 - accuracy: 0.8201\n",
      "Epoch 67/100\n",
      "1111679/1111679 [==============================] - 610s 549us/step - loss: 0.4161 - accuracy: 0.8198\n",
      "Epoch 68/100\n",
      "1111679/1111679 [==============================] - 610s 549us/step - loss: 0.4166 - accuracy: 0.8202\n",
      "Epoch 69/100\n",
      "1111679/1111679 [==============================] - 612s 550us/step - loss: 0.4165 - accuracy: 0.8199\n",
      "Epoch 70/100\n",
      "1111679/1111679 [==============================] - 614s 552us/step - loss: 0.4168 - accuracy: 0.8199\n",
      "Epoch 71/100\n",
      "1111679/1111679 [==============================] - 596s 536us/step - loss: 0.4165 - accuracy: 0.8199\n",
      "Epoch 72/100\n",
      "1111679/1111679 [==============================] - 598s 537us/step - loss: 0.4167 - accuracy: 0.8201\n",
      "Epoch 73/100\n",
      "1111679/1111679 [==============================] - 604s 543us/step - loss: 0.4166 - accuracy: 0.8201\n",
      "Epoch 74/100\n",
      "1111679/1111679 [==============================] - 609s 548us/step - loss: 0.4168 - accuracy: 0.8201\n",
      "Epoch 75/100\n",
      "1111679/1111679 [==============================] - 619s 557us/step - loss: 0.4169 - accuracy: 0.8200\n",
      "Epoch 76/100\n",
      "1111679/1111679 [==============================] - 616s 555us/step - loss: 0.4170 - accuracy: 0.8199\n",
      "Epoch 77/100\n",
      "1111679/1111679 [==============================] - 602s 542us/step - loss: 0.4173 - accuracy: 0.8200\n",
      "Epoch 78/100\n",
      "1111679/1111679 [==============================] - 612s 551us/step - loss: 0.4176 - accuracy: 0.8200\n",
      "Epoch 79/100\n",
      "1111679/1111679 [==============================] - 612s 551us/step - loss: 0.4176 - accuracy: 0.8198\n",
      "Epoch 80/100\n",
      "1111679/1111679 [==============================] - 612s 551us/step - loss: 0.4172 - accuracy: 0.8199\n",
      "Epoch 81/100\n",
      "1111679/1111679 [==============================] - 614s 553us/step - loss: 0.4176 - accuracy: 0.8198\n",
      "Epoch 82/100\n",
      "1111679/1111679 [==============================] - 612s 550us/step - loss: 0.4179 - accuracy: 0.8198\n",
      "Epoch 83/100\n",
      "1111679/1111679 [==============================] - 611s 549us/step - loss: 0.4178 - accuracy: 0.8197\n",
      "Epoch 84/100\n",
      "1111679/1111679 [==============================] - 613s 551us/step - loss: 0.4179 - accuracy: 0.8199\n",
      "Epoch 85/100\n",
      "1111679/1111679 [==============================] - 611s 550us/step - loss: 0.4175 - accuracy: 0.8197\n",
      "Epoch 86/100\n",
      "1111679/1111679 [==============================] - 610s 549us/step - loss: 0.4175 - accuracy: 0.8198\n",
      "Epoch 87/100\n",
      "1111679/1111679 [==============================] - 613s 551us/step - loss: 0.4176 - accuracy: 0.8199\n",
      "Epoch 88/100\n",
      "1111679/1111679 [==============================] - 611s 550us/step - loss: 0.4176 - accuracy: 0.8199\n",
      "Epoch 89/100\n",
      "1111679/1111679 [==============================] - 610s 549us/step - loss: 0.4177 - accuracy: 0.8198\n",
      "Epoch 90/100\n",
      "1111679/1111679 [==============================] - 609s 548us/step - loss: 0.4178 - accuracy: 0.8199\n",
      "Epoch 91/100\n",
      "1111679/1111679 [==============================] - 609s 548us/step - loss: 0.4179 - accuracy: 0.8198\n",
      "Epoch 92/100\n",
      "1111679/1111679 [==============================] - 610s 548us/step - loss: 0.4180 - accuracy: 0.8197\n",
      "Epoch 93/100\n",
      "1111679/1111679 [==============================] - 610s 549us/step - loss: 0.4180 - accuracy: 0.8195\n",
      "Epoch 94/100\n",
      "1111679/1111679 [==============================] - 609s 548us/step - loss: 0.4184 - accuracy: 0.8196\n",
      "Epoch 95/100\n",
      "1111679/1111679 [==============================] - 615s 553us/step - loss: 0.4182 - accuracy: 0.8198\n",
      "Epoch 96/100\n",
      "1111679/1111679 [==============================] - 612s 550us/step - loss: 0.4183 - accuracy: 0.8196\n",
      "Epoch 97/100\n",
      "1111679/1111679 [==============================] - 612s 551us/step - loss: 0.4186 - accuracy: 0.8197\n",
      "Epoch 98/100\n",
      "1111679/1111679 [==============================] - 610s 548us/step - loss: 0.4183 - accuracy: 0.8196\n",
      "Epoch 99/100\n",
      "1111679/1111679 [==============================] - 604s 543us/step - loss: 0.4183 - accuracy: 0.8196\n",
      "Epoch 100/100\n",
      "1111679/1111679 [==============================] - 600s 540us/step - loss: 0.4184 - accuracy: 0.8195\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-d5b00288626c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbooly_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy: %.2f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Build model with 2 FC layers each 50 nodes (relu activation), and 1 node output output layer (sigmoid activation)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, init='uniform', input_dim=128, activation='relu'))\n",
    "model.add(Dense(50, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init= 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "#Compiled with adam optimizer, binary crossentropy loss function, accuracy metric for evaluation\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#train the model over 100 epochs\n",
    "model.fit(encX_train, booly_train, epochs=100, batch_size=10)\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277920/277920 [==============================] - 16s 56us/step\n",
      "Accuracy: 81.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    225460\n",
      "           1       0.65      0.07      0.13     52460\n",
      "\n",
      "    accuracy                           0.82    277920\n",
      "   macro avg       0.74      0.53      0.51    277920\n",
      "weighted avg       0.79      0.82      0.75    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#see the classification report\n",
    "print(classification_report(booly_test, model.predict_classes(encX_test)))\n",
    "\n",
    "#Save the trained model into an .h5 file\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Hidden Layers: 128->75->50->25->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(75, input_dim=128, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1111679/1111679 [==============================] - 656s 590us/step - loss: 0.4239 - accuracy: 0.8166\n",
      "Epoch 2/25\n",
      "     90/1111679 [..............................] - ETA: 25:43 - loss: 0.4540 - accuracy: 0.7778  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111679/1111679 [==============================] - 657s 591us/step - loss: 0.4171 - accuracy: 0.8188\n",
      "Epoch 3/25\n",
      "1111679/1111679 [==============================] - 658s 592us/step - loss: 0.4155 - accuracy: 0.8193\n",
      "Epoch 4/25\n",
      "1111679/1111679 [==============================] - 681s 613us/step - loss: 0.4147 - accuracy: 0.8197\n",
      "Epoch 5/25\n",
      "1111679/1111679 [==============================] - 773s 695us/step - loss: 0.4143 - accuracy: 0.8198\n",
      "Epoch 6/25\n",
      "1111679/1111679 [==============================] - 794s 714us/step - loss: 0.4138 - accuracy: 0.8202\n",
      "Epoch 7/25\n",
      "1111679/1111679 [==============================] - 711s 639us/step - loss: 0.4137 - accuracy: 0.8202\n",
      "Epoch 8/25\n",
      "1111679/1111679 [==============================] - 639s 574us/step - loss: 0.4136 - accuracy: 0.8199\n",
      "Epoch 9/25\n",
      "1111679/1111679 [==============================] - 618s 556us/step - loss: 0.4136 - accuracy: 0.8201\n",
      "Epoch 10/25\n",
      "1111679/1111679 [==============================] - 618s 556us/step - loss: 0.4137 - accuracy: 0.8201\n",
      "Epoch 11/25\n",
      "1111679/1111679 [==============================] - 621s 559us/step - loss: 0.4135 - accuracy: 0.8201\n",
      "Epoch 12/25\n",
      "1111679/1111679 [==============================] - 617s 555us/step - loss: 0.4135 - accuracy: 0.8204\n",
      "Epoch 13/25\n",
      "1111679/1111679 [==============================] - 618s 556us/step - loss: 0.4135 - accuracy: 0.8202\n",
      "Epoch 14/25\n",
      "1111679/1111679 [==============================] - 636s 572us/step - loss: 0.4135 - accuracy: 0.8202\n",
      "Epoch 15/25\n",
      "1111679/1111679 [==============================] - 671s 604us/step - loss: 0.4136 - accuracy: 0.8201\n",
      "Epoch 16/25\n",
      "1111679/1111679 [==============================] - 671s 603us/step - loss: 0.4139 - accuracy: 0.8203\n",
      "Epoch 17/25\n",
      "1111679/1111679 [==============================] - 671s 604us/step - loss: 0.4137 - accuracy: 0.8203\n",
      "Epoch 18/25\n",
      "1111679/1111679 [==============================] - 673s 605us/step - loss: 0.4139 - accuracy: 0.8202\n",
      "Epoch 19/25\n",
      "1111679/1111679 [==============================] - 673s 605us/step - loss: 0.4139 - accuracy: 0.8203\n",
      "Epoch 20/25\n",
      "1111679/1111679 [==============================] - 670s 603us/step - loss: 0.4142 - accuracy: 0.8200\n",
      "Epoch 21/25\n",
      "1111679/1111679 [==============================] - 672s 605us/step - loss: 0.4136 - accuracy: 0.8203\n",
      "Epoch 22/25\n",
      "1111679/1111679 [==============================] - 650s 584us/step - loss: 0.4140 - accuracy: 0.8200\n",
      "Epoch 23/25\n",
      "1111679/1111679 [==============================] - 634s 570us/step - loss: 0.4142 - accuracy: 0.8202\n",
      "Epoch 24/25\n",
      "1111679/1111679 [==============================] - 634s 571us/step - loss: 0.4148 - accuracy: 0.8201\n",
      "Epoch 25/25\n",
      "1111679/1111679 [==============================] - 629s 566us/step - loss: 0.4145 - accuracy: 0.8202\n",
      "277920/277920 [==============================] - 16s 58us/step\n",
      "Accuracy: 81.93\n",
      "277920/277920 [==============================] - 16s 57us/step\n",
      "Accuracy: 81.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    225430\n",
      "           1       0.65      0.09      0.17     52490\n",
      "\n",
      "    accuracy                           0.82    277920\n",
      "   macro avg       0.74      0.54      0.53    277920\n",
      "weighted avg       0.79      0.82      0.76    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Build model with 2 FC layers each 50 nodes (relu activation), and 1 node output output layer (sigmoid activation)\n",
    "model = Sequential()\n",
    "model.add(Dense(75, init='uniform', input_dim=128, activation='relu'))\n",
    "model.add(Dense(50, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init= 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "#setup earlystop callback\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=3)\n",
    "\n",
    "#Compiled with adam optimizer, binary crossentropy loss function, accuracy metric for evaluation\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#train the model over 100 epochs\n",
    "model.fit(encX_train, booly_train, epochs=25, batch_size=10, callbacks=[earlystop_callback])\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#see the classification report\n",
    "print(classification_report(booly_test, model.predict_classes(encX_test)))\n",
    "\n",
    "#Save the trained model into an .h5 file\n",
    "model.save(\"model2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Hidden Layers: 128->50->50->50->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, input_dim=128, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1111679 samples, validate on 277920 samples\n",
      "Epoch 1/50\n",
      "1111679/1111679 [==============================] - 940s 846us/step - loss: 0.4245 - accuracy: 0.8167 - val_loss: 0.4211 - val_accuracy: 0.8172\n",
      "Epoch 2/50\n",
      "1111679/1111679 [==============================] - 940s 846us/step - loss: 0.4190 - accuracy: 0.8183 - val_loss: 0.4205 - val_accuracy: 0.8178\n",
      "Epoch 3/50\n",
      "1111679/1111679 [==============================] - 942s 847us/step - loss: 0.4181 - accuracy: 0.8187 - val_loss: 0.4178 - val_accuracy: 0.8183\n",
      "Epoch 4/50\n",
      "1111679/1111679 [==============================] - 944s 850us/step - loss: 0.4179 - accuracy: 0.8190 - val_loss: 0.4197 - val_accuracy: 0.8166\n",
      "Epoch 5/50\n",
      "1111679/1111679 [==============================] - 947s 852us/step - loss: 0.4181 - accuracy: 0.8191 - val_loss: 0.4213 - val_accuracy: 0.8174\n",
      "Epoch 6/50\n",
      "1111679/1111679 [==============================] - 948s 853us/step - loss: 0.4186 - accuracy: 0.8190 - val_loss: 0.4181 - val_accuracy: 0.8188\n",
      "Epoch 7/50\n",
      "1111679/1111679 [==============================] - 949s 854us/step - loss: 0.4189 - accuracy: 0.8189 - val_loss: 0.4249 - val_accuracy: 0.8179\n",
      "Epoch 8/50\n",
      "1111679/1111679 [==============================] - 952s 856us/step - loss: 0.4191 - accuracy: 0.8186 - val_loss: 0.4198 - val_accuracy: 0.8184\n",
      "Epoch 9/50\n",
      "1111679/1111679 [==============================] - 957s 860us/step - loss: 0.4195 - accuracy: 0.8185 - val_loss: 0.4190 - val_accuracy: 0.8182\n",
      "277920/277920 [==============================] - 10s 36us/step\n",
      "Accuracy: 81.82\n",
      "277920/277920 [==============================] - 10s 36us/step\n",
      "Accuracy: 81.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90    225272\n",
      "           1       0.59      0.13      0.22     52648\n",
      "\n",
      "    accuracy                           0.82    277920\n",
      "   macro avg       0.71      0.56      0.56    277920\n",
      "weighted avg       0.78      0.82      0.77    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Build model with 2 FC layers each 50 nodes (relu activation), and 1 node output output layer (sigmoid activation)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, init='uniform', input_dim=128, activation='relu'))\n",
    "model.add(Dense(50, init='uniform', activation='relu'))\n",
    "model.add(Dense(50, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init= 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "#Compiled with adam optimizer, binary crossentropy loss function, accuracy metric for evaluation\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Setting up logging for tensorboard\n",
    "#log_dir =\"logs/\" #+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#setup earlystop callback\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=3)\n",
    "\n",
    "#train the model over 100 epochs, with validation data, and tensorbard and early stopping callbacks added\n",
    "model.fit(encX_train, booly_train, \n",
    "          epochs=50, batch_size=5, \n",
    "          validation_data=(encX_test, booly_test), \n",
    "          callbacks =[earlystop_callback])\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#see the classification report\n",
    "print(classification_report(booly_test, model.predict_classes(encX_test)))\n",
    "\n",
    "#Save the trained model into an .h5 file\n",
    "model.save(\"model50-50-50.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Hidden Layers: 128->100->100->100->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, input_dim=128, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1111679 samples, validate on 277920 samples\n",
      "Epoch 1/50\n",
      "1111679/1111679 [==============================] - 945s 850us/step - loss: 0.4243 - accuracy: 0.8167 - val_loss: 0.4193 - val_accuracy: 0.8180\n",
      "Epoch 2/50\n",
      "1111679/1111679 [==============================] - 922s 829us/step - loss: 0.4186 - accuracy: 0.8185 - val_loss: 0.4179 - val_accuracy: 0.8181\n",
      "Epoch 3/50\n",
      "1111679/1111679 [==============================] - 917s 825us/step - loss: 0.4176 - accuracy: 0.8191 - val_loss: 0.4199 - val_accuracy: 0.8183\n",
      "Epoch 4/50\n",
      "1111679/1111679 [==============================] - 917s 825us/step - loss: 0.4176 - accuracy: 0.8191 - val_loss: 0.4191 - val_accuracy: 0.8188\n",
      "Epoch 5/50\n",
      "1111679/1111679 [==============================] - 915s 823us/step - loss: 0.4174 - accuracy: 0.8192 - val_loss: 0.4177 - val_accuracy: 0.8185\n",
      "Epoch 6/50\n",
      "1111679/1111679 [==============================] - 917s 825us/step - loss: 0.4182 - accuracy: 0.8194 - val_loss: 0.4193 - val_accuracy: 0.8190\n",
      "Epoch 7/50\n",
      "1111679/1111679 [==============================] - 951s 855us/step - loss: 0.4191 - accuracy: 0.8191 - val_loss: 0.4174 - val_accuracy: 0.8189\n",
      "Epoch 8/50\n",
      "1111679/1111679 [==============================] - 956s 860us/step - loss: 0.4184 - accuracy: 0.8194 - val_loss: 0.4182 - val_accuracy: 0.8187\n",
      "Epoch 9/50\n",
      "1111679/1111679 [==============================] - 957s 861us/step - loss: 0.4189 - accuracy: 0.8191 - val_loss: 0.4193 - val_accuracy: 0.8187\n",
      "277920/277920 [==============================] - 10s 35us/step\n",
      "Accuracy: 81.87\n",
      "277920/277920 [==============================] - 10s 35us/step\n",
      "Accuracy: 81.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.90    225272\n",
      "           1       0.58      0.15      0.24     52648\n",
      "\n",
      "    accuracy                           0.82    277920\n",
      "   macro avg       0.71      0.56      0.57    277920\n",
      "weighted avg       0.78      0.82      0.77    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Build model with 2 FC layers each 50 nodes (relu activation), and 1 node output output layer (sigmoid activation)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, init='uniform', input_dim=128, activation='relu'))\n",
    "model.add(Dense(100, init='uniform', activation='relu'))\n",
    "model.add(Dense(100, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init= 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "#Compiled with adam optimizer, binary crossentropy loss function, accuracy metric for evaluation\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Setting up logging for tensorboard\n",
    "#log_dir =\"logs/\" #+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#setup earlystop callback\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=3)\n",
    "\n",
    "#train the model over 100 epochs, with validation data, and tensorbard and early stopping callbacks added\n",
    "model.fit(encX_train, booly_train, \n",
    "          epochs=50, batch_size=5, \n",
    "          validation_data=(encX_test, booly_test), \n",
    "          callbacks =[earlystop_callback])\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#see the classification report\n",
    "print(classification_report(booly_test, model.predict_classes(encX_test)))\n",
    "\n",
    "#Save the trained model into an .h5 file\n",
    "model.save(\"model100-100-100.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Hidden Layers: 128->100->100->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, input_dim=128, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1111679 samples, validate on 277920 samples\n",
      "Epoch 1/50\n",
      "1111679/1111679 [==============================] - 929s 836us/step - loss: 0.4236 - accuracy: 0.8171 - val_loss: 0.4238 - val_accuracy: 0.8182\n",
      "Epoch 2/50\n",
      "1111679/1111679 [==============================] - 958s 862us/step - loss: 0.4188 - accuracy: 0.8188 - val_loss: 0.4206 - val_accuracy: 0.8187\n",
      "Epoch 3/50\n",
      "1111679/1111679 [==============================] - 889s 800us/step - loss: 0.4177 - accuracy: 0.8191 - val_loss: 0.4186 - val_accuracy: 0.8184\n",
      "Epoch 4/50\n",
      "1111679/1111679 [==============================] - 883s 794us/step - loss: 0.4171 - accuracy: 0.8194 - val_loss: 0.4178 - val_accuracy: 0.8183\n",
      "Epoch 5/50\n",
      "1111679/1111679 [==============================] - 883s 795us/step - loss: 0.4168 - accuracy: 0.8194 - val_loss: 0.4193 - val_accuracy: 0.8189\n",
      "Epoch 6/50\n",
      "1111679/1111679 [==============================] - 883s 794us/step - loss: 0.4168 - accuracy: 0.8196 - val_loss: 0.4179 - val_accuracy: 0.8190\n",
      "Epoch 7/50\n",
      "1111679/1111679 [==============================] - 884s 795us/step - loss: 0.4167 - accuracy: 0.8194 - val_loss: 0.4205 - val_accuracy: 0.8179\n",
      "Epoch 8/50\n",
      "1111679/1111679 [==============================] - 883s 794us/step - loss: 0.4168 - accuracy: 0.8196 - val_loss: 0.4187 - val_accuracy: 0.8190\n",
      "Epoch 9/50\n",
      "1111679/1111679 [==============================] - 884s 795us/step - loss: 0.4168 - accuracy: 0.8197 - val_loss: 0.4185 - val_accuracy: 0.8186\n",
      "277920/277920 [==============================] - 9s 34us/step\n",
      "Accuracy: 81.86\n",
      "277920/277920 [==============================] - 9s 34us/step\n",
      "Accuracy: 81.86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    225272\n",
      "           1       0.64      0.09      0.16     52648\n",
      "\n",
      "    accuracy                           0.82    277920\n",
      "   macro avg       0.73      0.54      0.53    277920\n",
      "weighted avg       0.79      0.82      0.76    277920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Build model with 2 FC layers each 50 nodes (relu activation), and 1 node output output layer (sigmoid activation)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, init='uniform', input_dim=128, activation='relu'))\n",
    "model.add(Dense(100, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init= 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "#Compiled with adam optimizer, binary crossentropy loss function, accuracy metric for evaluation\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Setting up logging for tensorboard\n",
    "#log_dir =\"logs/\" #+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#setup earlystop callback\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=3)\n",
    "\n",
    "#train the model over 100 epochs, with validation data, and tensorbard and early stopping callbacks added\n",
    "model.fit(encX_train, booly_train, \n",
    "          epochs=50, batch_size=5, \n",
    "          validation_data=(encX_test, booly_test), \n",
    "          callbacks =[earlystop_callback])\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, booly_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#see the classification report\n",
    "print(classification_report(booly_test, model.predict_classes(encX_test)))\n",
    "\n",
    "#Save the trained model into an .h5 file\n",
    "model.save(\"model100-100.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "From the results of the DNN models, it seems like we did not achieve an optimal predictor, however, since training was done locally with my computer (GTX 1080 GPU w/ 8GRAM), we could experiment to far. It seems like if a much bigger network was made, it could get better result, but this would seem impractical because of how long it would take to train, but also it would have a hard time trying to keep up with the results for the Logistic Regression. The logisitic regression by far outpreformed all other models, most models having abysmal testing accuracy. However, it should still be ackknowledged that the Logistic Regression was medicore at best with its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Resolutio Classifier\n",
    "\n",
    "This is the third notebook in the series of notbooks designing the best predictors of reslutions for fincacial complaints submitted to the Consumer Finacial Complaint Protection Bureau (CFCB). \n",
    "\n",
    "In the previous notebook, our classifier models were designed to predict if complaints would be entitled to some sort of relief or not, in a binary classification. In this notebook, we run similar models, but instead have an output that can be specified as one of 6 different categories.\n",
    "\n",
    "As stated in the last notebook, to understand how and why the data was trasformed below, check the data explortion notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Defining what dtype to convert each column to\n",
    "#numberic columns are transformed after reading in\n",
    "dtype_dict = {'Product':\"category\",\n",
    "             'Consumer consent provided?': \"category\",\n",
    "             'Submitted via': \"category\",\n",
    "             'Company response to consumer': \"category\",\n",
    "             'Consumer disputed?': \"category\"}\n",
    "\n",
    "#read in .csv file, dates are parsed into datetime objects. \n",
    "#The Complaint ID is Unique in every entry, so it can be used as index\n",
    "df = pd.read_csv('Consumer_Complaints.csv',\n",
    "                 index_col=['Complaint ID'],\n",
    "                 parse_dates=[\"Date received\",\"Date sent to company\"],\n",
    "                 dtype=dtype_dict)\n",
    "\n",
    "#This will replace ending '-' to 5 (average linespace of 10)\n",
    "regexReplaceDash = r\"(\\d+)(-)$\"\n",
    "df['ZIP code'] = df['ZIP code'].str.replace(regexReplaceDash, r'\\g<1>5')\n",
    "\n",
    "#This will change ending XX to 50 (average linespace of 100)\n",
    "regex_XX = r'(\\d{3})(XX)'\n",
    "df['ZIP code'] = df['ZIP code'].str.replace(regex_XX, r'\\g<1>50')\n",
    "\n",
    "#This will remove all other entries that are still not 5 digits\n",
    "regexRemove = r'\\D+'\n",
    "df['ZIP code'] = df['ZIP code'].replace(regexRemove, np.nan, regex=True)\n",
    "\n",
    "#imputes the mean for nan \n",
    "imputeMean = df['ZIP code'].astype(np.float).mean()\n",
    "df['ZIP code'] = df['ZIP code'].astype(np.float).fillna(imputeMean)\n",
    "\n",
    "#Transforming 2 unique valued col to float boolean\n",
    "booleanize = {'Yes': 1, 'No': 0}\n",
    "df['Timely response?'] = pd.Series(df['Timely response?'].map(booleanize), dtype = np.float)\n",
    "\n",
    "#function to apply to column to convert less common results to 'Other', as well as NaN\n",
    "def convertToOther(value, keepList):\n",
    "    if (value == ''):\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        return value if value in keepList else \"Other\"\n",
    "    \n",
    "#Lists top 23 value counts (allowed to exclude values), turns NaN to '' to others, converts to category dtype\n",
    "def cleanReduceConvert(df, column, blackList=[]):\n",
    "    keepList = []\n",
    "    for category in df[column].value_counts().head(23).index.tolist():\n",
    "        if (category.lower().split()[0] != \"other\"):\n",
    "            keepList.append(category)\n",
    "    for category in blackList:\n",
    "        try:\n",
    "            keepList.remove(category)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    df[column].fillna('', inplace=True)\n",
    "    return pd.Series(df[column].apply(convertToOther, args=(keepList,)), dtype = 'category')\n",
    "\n",
    "df['Sub-product'] = cleanReduceConvert(df, 'Sub-product', blackList= ['I do not know'])\n",
    "df['Issue'] = cleanReduceConvert(df, 'Issue')\n",
    "df['Sub-issue'] = cleanReduceConvert(df, 'Sub-issue')\n",
    "df['Company'] = cleanReduceConvert(df, 'Company')\n",
    "\n",
    "def entryOrNull(strVal):\n",
    "    return 1.0 if strVal is not np.nan else 0.0\n",
    "\n",
    "df['Consumer complaint narrative submitted?'] = df['Consumer complaint narrative'].apply(entryOrNull)\n",
    "\n",
    "def dtToCols(df, dtcolumn):\n",
    "    df[\"{} day\".format(dtcolumn)] = df[dtcolumn].dt.day\n",
    "    df[\"{} month\".format(dtcolumn)] = df[dtcolumn].dt.month\n",
    "    df[\"{} year\".format(dtcolumn)] = df[dtcolumn].dt.year\n",
    "    \n",
    "dtToCols(df, \"Date received\")\n",
    "dtToCols(df, \"Date sent to company\")\n",
    "\n",
    "df[\"Consumer consent provided?\"] = df[\"Consumer consent provided?\"].cat.add_categories(\"Not recorded\").fillna(\"Not recorded\")\n",
    "\n",
    "df = df.drop(df[df[\"Company response to consumer\"].isna()].index)\n",
    "\n",
    "dfInProgress = df[df[\"Company response to consumer\"] == \"In progress\"]\n",
    "df = df[df[\"Company response to consumer\"] != \"In progress\"]\n",
    "\n",
    "dfUntimelyResponse = df[df[\"Company response to consumer\"] == \"Untimely response\"]\n",
    "df = df[df[\"Company response to consumer\"] != \"Untimely response\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#data columns not be used for the model\n",
    "dropList = [\"Consumer complaint narrative\",\n",
    "            \"Company public response\",\n",
    "            \"State\",\n",
    "            \"Tags\",\n",
    "            \"Consumer disputed?\",\n",
    "            \"Date received\", \n",
    "            \"Date sent to company\",\n",
    "            \"Company response to consumer\"]\n",
    "X = df.drop(dropList, axis=1)\n",
    "Y = df[\"Company response to consumer\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "#Columns to be standard scaled/imputed\n",
    "numeric_features = ['ZIP code',\n",
    "                    'Date received day',\n",
    "                    'Date received month',\n",
    "                    'Date received year',\n",
    "                    'Date sent to company day',\n",
    "                    'Date sent to company month',\n",
    "                    'Date sent to company year']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "#Columns to one hot encoded\n",
    "categorical_features = ['Product',\n",
    "           'Sub-product',\n",
    "           'Issue',\n",
    "           'Sub-issue',\n",
    "           'Company',\n",
    "           'Consumer consent provided?',\n",
    "           'Submitted via',\n",
    "           'Timely response?']\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "#building the column transformer with both transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "#fit the preprocessor, then transform trainging and test set, assign sparse matrix to variables\n",
    "preprocessor.fit(X)\n",
    "encX_train = preprocessor.transform(X_train)\n",
    "encX_test = preprocessor.transform(X_test)\n",
    "\n",
    "#Creating Dummy variable for target. Will be neccessary for DNN. \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "dummy_y_train = np_utils.to_categorical(encoder.transform(y_train))\n",
    "dummy_y_test = np_utils.to_categorical(encoder.transform(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below allows the feature extraction to be labeled for easier interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names_from_ColumnTransformer(column_transformer):    \n",
    "    col_name = []\n",
    "    for transformer_in_columns in column_transformer.transformers_[:-1]:#the last transformer is ColumnTransformer's 'remainder'\n",
    "        raw_col_name = transformer_in_columns[2]\n",
    "        if isinstance(transformer_in_columns[1],Pipeline): \n",
    "            transformer = transformer_in_columns[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = transformer_in_columns[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError: # if no 'get_feature_names' function, use raw column name\n",
    "            names = raw_col_name\n",
    "        if isinstance(names,np.ndarray): # eg.\n",
    "            col_name += names.tolist()\n",
    "        elif isinstance(names,list):\n",
    "            col_name += names    \n",
    "        elif isinstance(names,str):\n",
    "            col_name.append(names)\n",
    "    return col_name\n",
    "\n",
    "processedColumns = get_column_names_from_ColumnTransformer(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Note that the classifier instance is created with the multi_class keyword argument set to 'mutlinomial' for self explanatory reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79930675494147\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                         Closed       0.00      0.00      0.00      5325\n",
      "        Closed with explanation       0.81      0.99      0.89    327800\n",
      "    Closed with monetary relief       0.51      0.02      0.03     23743\n",
      "Closed with non-monetary relief       0.52      0.07      0.13     53102\n",
      "             Closed with relief       0.48      0.11      0.18      1620\n",
      "          Closed without relief       0.76      0.93      0.83      5290\n",
      "\n",
      "                       accuracy                           0.80    416880\n",
      "                      macro avg       0.51      0.35      0.34    416880\n",
      "                   weighted avg       0.74      0.80      0.73    416880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1, solver='saga', penalty='l1', multi_class='multinomial')\n",
    "lr_para = {'C':[1.0,0.1,0.01], \n",
    "           'class_weight':[None,'balanced'],\n",
    "           'max_iter':[50,100,125]}\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(lr, lr_para,cv=5, scoring='accuracy', n_jobs=-1)\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting LogisiticRegression(), create prediciton from X_test data\n",
    "bestfitLR = fitmodel.best_estimator_\n",
    "\n",
    "bestfitLR.fit(encX_train,y_train)\n",
    "y_pred = bestfitLR.predict(encX_test)\n",
    "print(bestfitLR.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes\n",
    "\n",
    "Bernoulli Naive Bayes Classifiers output a predicition vector that has a normalize probabilities of each possible outcome. A piculiar trait of this model is that it treats the contribution to the probabilty from each feature completely seperate, assuming that all features are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  72 | elapsed:   34.6s remaining:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  72 | elapsed:   36.2s remaining:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of  72 | elapsed:   40.8s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   41.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4593959892535022\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                         Closed       0.06      0.36      0.10      5325\n",
      "        Closed with explanation       0.87      0.45      0.59    327800\n",
      "    Closed with monetary relief       0.21      0.62      0.31     23743\n",
      "Closed with non-monetary relief       0.27      0.45      0.34     53102\n",
      "             Closed with relief       0.06      0.48      0.11      1620\n",
      "          Closed without relief       0.08      0.63      0.14      5290\n",
      "\n",
      "                       accuracy                           0.46    416880\n",
      "                      macro avg       0.26      0.50      0.26    416880\n",
      "                   weighted avg       0.74      0.46      0.53    416880\n",
      "\n",
      "BernoulliNB(alpha=10, binarize=0.1, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb_para = {'alpha':[1,2,10,0],\n",
    "            'fit_prior': [True,False],\n",
    "            'binarize': [0,0.1, 0.5]}\n",
    "\n",
    "#Apply grid search with above parameters specified\n",
    "fitmodel = GridSearchCV(bnb, bnb_para,cv=3, scoring='accuracy', n_jobs=-1, verbose=10)\n",
    "fitmodel.fit(encX_train,y_train)\n",
    "\n",
    "#store the best fitting GradientBoostingClassifier(), create prediciton from X_test data\n",
    "bestfitBNB = fitmodel.best_estimator_\n",
    "\n",
    "bestfitBNB.fit(encX_train,y_train)\n",
    "y_pred = bestfitBNB.predict(encX_test)\n",
    "print(bestfitBNB.score(encX_test,y_test))\n",
    "\n",
    "#display the result\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(bestfitBNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN classifier\n",
    "\n",
    "Mostly as an excercise, a Deep Neural Network Classifier was built using similar architech from the last notebook. The results were not too promising. Note that the cell spat an error after training, so saving was done in a different cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Build model with 2 FC layers each 50 nodes (relu activation), and 1 node output output layer (sigmoid activation)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, init='uniform', input_dim=128, activation='relu'))\n",
    "model.add(Dense(50, init='uniform', activation='relu'))\n",
    "model.add(Dense(6, init= 'uniform', activation = 'softmax'))\n",
    "\n",
    "#setup earlystop callback\n",
    "earlystop_callback = EarlyStopping(monitor='accuracy', min_delta=0.0001, patience=3)\n",
    "\n",
    "#Compiled with adam optimizer, binary crossentropy loss function, accuracy metric for evaluation\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#train the model over 100 epochs\n",
    "model.fit(encX_train, dummy_y_train, epochs=10, batch_size=10, callbacks=[earlystop_callback])\n",
    "\n",
    "#print the results\n",
    "_, accuracy = model.evaluate(encX_test, dummy_y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#Save the trained model into an .h5 file\n",
    "model.save(\"multiclass-model-50-50.h5\")\n",
    "\n",
    "#see the classification report\n",
    "#print(classification_report(dummy_y_test, model.predict_classes(encX_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the trained model into an .h5 file\n",
    "model.save(\"multiclass-model-50-50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
